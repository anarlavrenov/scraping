# 1. Нужно в терминале установить скрапи: pip install scrapy
# 2. В терминале создать проект уже для скрапи: scrapy startproject jobparser .
# 3. Нужно создать паука.
# Первым аргументом передается название сайта, вторым - домен второго уровня: scrapy genspider djinnico djinni.co

# Файл settings:
    # - BOT_NAME - хранит найменование проекта
    # - SPIDER_MODULES, NEWSPIDER_MODULE - нужно для указания и управления директориями пауков.
    # SPIDER_MODULES - содержит указание директорий, в которых будут находиться пауки
    # NEWSPIDER_MODULE - это директория, в которой будут создаваться все новые пауки при вызове scrapy genspider...
    # USER_AGENT - Сюда нужно разово вписать user agent
    # ROBOTSTXT_OBEY - нужно выключить, прописав False
    # CONCURRENT_REQUESTS - к-во запросов на сервер за раз, нужно раскомментировать
    #  - По дефолту стоит 16, если не раскомментировать. Поставлю 16 - низко-среднюю агрессивность, чтобы не заблокировали
    # DOWNLOAD_DELAY - пауза в секундах. Паук сделал 16 запросов - 3 секунды отдохнул, потом еще 16. Нужно
    #  - раскомментировать. Поставил 0.5
    # COOKIES_ENABLED - позволяет сохранять куки после первого захода и уже работать с их учетом в дальнейшем.
        # - Например, сервер позволит сохранять авторизацию
        # - Поставил True
    # LOG_ENABLED, LOG_LEVEL - включаю логи и указываю их уровень. Самый масштабный - это DEBUG
        # - Изначально этих 2 параметров не было. Вписал сам. Нужно для того, чтобы показывались ошибки

# scrapy crawl djinnico - запустить скрапер через терминал. djinnico - значение атрибута name в djinnico.py

# Для того, чтобы собрать данные из разных сайтов - нужно создавать несколько пауков,
# чтобы собрать данные по примеру текущей работы - достаточно одного, так как структура двух страниц для Украины и ЕС
# одинаковая